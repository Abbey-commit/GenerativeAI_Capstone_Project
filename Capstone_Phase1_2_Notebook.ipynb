{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a5c58b3",
   "metadata": {},
   "source": [
    "# üìò Phase 1: Fetching and Preprocessing PubMed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afcd62a",
   "metadata": {},
   "source": [
    "\n",
    "In this section, we fetch biomedical content relevant to mental health disorders ‚Äî specifically **depression**, **psychosis**, and **anxiety** ‚Äî from PubMed via the Entrez API. We preprocess the text to remove noise and prepare it for vector embedding.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2083df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q biopython nltk\n",
    "from Bio import Entrez\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e23ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure Entrez email\n",
    "Entrez.email = \"abiodunadebisi614@gmail.com\"  # Replace with your email for Entrez access\n",
    "\n",
    "# Search and fetch PubMed abstracts related to mental health disorders\n",
    "def fetch_pubmed_abstracts(query, max_results=10):\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
    "    record = Entrez.read(handle)\n",
    "    id_list = record[\"IdList\"]\n",
    "    handle.close()\n",
    "\n",
    "    abstracts = []\n",
    "    if id_list:\n",
    "        handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(id_list), rettype=\"abstract\", retmode=\"text\")\n",
    "        abstracts = handle.read().split(\"\\n\\n\")\n",
    "        handle.close()\n",
    "    return abstracts\n",
    "\n",
    "# Fetch sample data\n",
    "abstracts = fetch_pubmed_abstracts(\"depression OR psychosis OR anxiety\")\n",
    "len(abstracts), abstracts[:2]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ab3e6",
   "metadata": {},
   "source": [
    "### üîç Clean and Normalize the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bd07d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,;:!?()\\-\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "cleaned_abstracts = [clean_text(abs) for abs in abstracts if abs.strip()]\n",
    "cleaned_abstracts[:2]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245fcae3",
   "metadata": {},
   "source": [
    "### ‚úÇÔ∏è Split into Chunks for Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c09036",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def chunk_text(text, max_length=500):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks, current_chunk = [], \"\"\n",
    "    for sent in sentences:\n",
    "        if len(current_chunk) + len(sent) <= max_length:\n",
    "            current_chunk += \" \" + sent\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sent\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "text_chunks = []\n",
    "for doc in cleaned_abstracts:\n",
    "    text_chunks.extend(chunk_text(doc))\n",
    "\n",
    "len(text_chunks), text_chunks[:3]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c80df4",
   "metadata": {},
   "source": [
    "# üìò Phase 2: Embedding and FAISS Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b297d9b3",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have cleaned and chunked the text data, we convert each chunk into vector embeddings using a pre-trained model from `sentence-transformers`. Then, we store the vectors in **FAISS**, a high-performance similarity search library.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b1196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q faiss-cpu sentence-transformers\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a lightweight biomedical transformer\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Encode the chunks\n",
    "embeddings = model.encode(text_chunks, show_progress_bar=True)\n",
    "embeddings = np.array(embeddings).astype(\"float32\")\n",
    "embeddings.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc106898",
   "metadata": {},
   "source": [
    "### üß† Index the Embeddings Using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "index.ntotal\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80155204",
   "metadata": {},
   "source": [
    "### üîç Sample Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090f5e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Query with a new sentence\n",
    "query = \"drugs for treating severe depression\"\n",
    "query_vector = model.encode([query]).astype(\"float32\")\n",
    "\n",
    "# Search\n",
    "top_k = 5\n",
    "distances, indices = index.search(query_vector, top_k)\n",
    "\n",
    "print(\"üîé Top retrieved chunks:\")\n",
    "for idx in indices[0]:\n",
    "    print(\"-\", text_chunks[idx])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
